---
title: "Predicting Train Arrival Status - On Time or Late"
author: "Adanna Alutu"
date: "June 6, 2017"
output: pdf_document
---

---
subtitle: <h1>Introduction</h1>
---

At the beginning of the project, it was hard to come up with a good data to analyze and predict the outcome. Initially I wanted to work on data from my job but after we coudn't see much dependence among the fields that made sense, my mentor Dr. Shmuel Naaman advised me to scout for data from other internet sites he recommended.

Since I take the train most of the time and experienced delay issues many times that has ranged from 10 mins to 2 hours, I became interested in working on transportation data for trains. This is because I want to experience the process of predicting outcomes which is made possible through Data Science. I want to focus on the steps that will make it possible for me and my mentor Dr Shmuel Naaman to predict the arrival times of the train. The possibility of cutting down the delays experienced in waiting for the train no longer seems to be far fetched.
My mentor agreed with me and the Septa Train data from Kaggle website was a good option to work on. There were 3 different datasets available to work on but I chose the "on time performance" which I felt has more relevant features, variables and observations and also has sufficient data for the analysis, tests involved.

The variables in the dataset include:
1. train_id
2. status
3. origin
4. direction
5. next_station
6. timeStamp
7. date

subtitle: <h1>Data Exploration</h1>

Several steps were taken to ensure elaborate data analysis and wrangling. Every bit of the data was maximized. We went beyond using the provided variables by creating new ones, removing unnecessary data and testing with reliable tools to get quality, reliable results that can be tested with any dataset. 

It was necessary to take the following steps to ensure that all the combinations, dicing and testing would yield a meaningful interpretation and prediction that will help tell us with high confidence when the train will be late:

I. We first tried to plot charts with the entire data but the plots were too crowded and blurry to make any sense. The scales were distorted with big units affected by the outliers.

II. GGPlot bar charts were used to plot and observe the trends and statistics summary but the dataset was too huge for the charts.

III. My mentor suggested shuffling the data and taking the first 20percent as sample to work on. Using the formula below, the row-wise shuffling was done first before the column was then shuffled:

```{r, echo=FALSE, eval=FALSE}

#first do row-wise shuffling
train_data_shflr <- train_data[sample(nrow(train_data)),]

train_data_shflc <- train_data_shflr[,sample(ncol(train_data_shflr))]

```

IV. We used the data to fit in several models which include:
    + GGPlot with different combination of the variables.
    + Linear regression model which was used different ways to get the best stastical summary. Including using some of the observations as variables.
    + CART model with focus on the classification method because most of the variables in the data are categorical and the prediction is binary with 0 as "on Time" and 1 as "Late"
    + Random Forest which created it's own model that highlighted the top more meaningful variables that contributed majorly in predicting the outcome. 

Each of these models were implemented because the train dataset contains a mixture of numerical and categorical variables. Converting their types to either numeric or factors wasn't sufficient. To get the benefit of all the variables, it was essential to test these models.


subtitle: <h1>Data Wrangling</h1>

Some data manipulations were done which include:
 + splitting some of the original variables nto separate variables. For example, time stamp variable was split into six variables. year, month day, hour, min, seconds.
 + Irrelevant variables were removed or set to null so they would not appear in the dataframe used for the predictions.
 + Some of observations from the wkday and day of month variables were converted to variables and they significantly improved the statistics of the models. The additions however increased the number of variables from 11 to 58.
 + Units attached to the dependent variable observations were removed to enable convesions to different types and allow plotting with only the observations of the same type.
 + The dependent variable "status" observations of "on Time" were replaced with "0" using gsub so that all the observations for the variable will match and easier to manipulate."On time" meant the train arrived as scheduled so it made sense to use "0" to represent no delay.
 
 
subtitle: <h1>A Peek into some new variables</h1> 
 
This section shows the summary of the SEPTA train data and the first few records using the head().

```{r, echo=FALSE}

library(tidyr)
library(lubridate)
library(broom)

library(ggplot2)

train_data <- read.csv("train100000.csv")

train_data$datets <- as.POSIXct(train_data$timeStamp, format = "%m/%d/%Y %H:%M")

train_data$wkday <-weekdays(as.Date(train_data$date, format = "%m/%d/%Y"))

train_data$month <-month(as.Date(train_data$date, "%m/%d/%Y"))


train_data <- separate(train_data, datets, c("date3","time"), sep = " ")

train_data <- separate(train_data, time, c("hour","minute"), sep = ":")


train_data <- separate(train_data, date3, c("date3yr","d3month", "monthday"), sep = "-")


#remove the date column
train_data$date <- NULL
train_data$date3yr <- NULL
train_data$d3month <- NULL
train_data$timeStamp <- NULL


train_data$status <- gsub(pattern="min", replacement = "", x = train_data$status, ignore.case = TRUE)

train_data$status <- gsub(pattern="On Time", replacement = "0", x = train_data$status, ignore.case = TRUE)

train_data$status <- log(as.numeric( train_data$status)+1)

train_data <- cbind(train_data, as.data.frame(model.matrix(~ wkday + monthday, data = train_data, contrasts.arg = list(wkday = contr.treatment(n = 7, contrasts = FALSE), monthday = contr.treatment(n = 31, contrasts = FALSE)))))
train_data$monthday <- as.numeric(train_data$monthday)

Data <- (train_data[c('status','origin', 'hour',  'minute', 'month' , 'wkday1' , 'wkday2' , 'wkday3' , 'wkday4' , 'wkday5' , 'wkday6' , 'wkday7' , 'monthday1' , 'monthday2' , 'monthday3' , 'monthday4' , 'monthday5' , 'monthday6' , 'monthday7' , 'monthday8' , 'monthday9' , 'monthday10' , 'monthday11' , 'monthday12' , 'monthday13' , 'monthday14', 'monthday15' , 'monthday16' , 'monthday17' , 'monthday18' , 'monthday19' , 'monthday20' , 'monthday21', 'monthday22' , 'monthday23' , 'monthday24' , 'monthday25' , 'monthday26' , 'monthday27' , 'monthday28', 'monthday29' , 'monthday30' , 'monthday31' ,  'next_station' ,  'direction')])
#head(train_data)
str(Data)
 


```


subtitle: <h1>CART Model /Decision Tree</h1>:

 In this section, the CART model is implemented. The two options considered are Classification and Regression CART models/trees but the reression model is preferred so that the results can be compared with the linear regression model used above. It's like comparing apples to apples or oranges to oranges.

I found this site very helpful because they explained in detail the conditions for the variables before a succesful model can be achieved - <https://rstudio-pubs-static.s3.amazonaws.com/27179_e64f0de316fc4f169d6ca300f18ee2aa.html>. 

Prior to finding this site, only the root or just one circle with a number (4.6) was drawn.


```{r}


library(caTools)
set.seed(3000)

smp_size <- floor (0.8 *nrow(Data))
 
train_ind <- sample (seq_len(nrow(Data)), size=smp_size)

train <- Data[train_ind, ]
test  <- Data[-train_ind,]
 

#build CART model

library(rpart)
library(rpart.plot)

#now create the CART model. Use rpart to build a linear regression tree since the status variable being predicted is a continous variable.

#use rpart formula to fit the data.

#logvar <- log(as.numeric( train_data$status)+1)
TraindataTree2 = rpart(status ~. , data = train)



actual <- test$status
predicted <- predict(TraindataTree2, newdata = test )

R2_test <- 1 - (sum((actual-predicted )^2)/sum((actual-mean(actual))^2))
 


actual <- train$status
predicted <- predict(TraindataTree2, newdata = train )

R2_train <- 1 - (sum((actual-predicted )^2)/sum((actual-mean(actual))^2))
R2_test
R2_train
#print R2




```





Check r-squared for Training and Test data for Regression model
```{r}
set.seed(2)

rm(R2lmtrn)
rm(R2lmtst)
rm(jj)
x <- .01 * 150000
#j
R2lmtrn <- 0
R2lmtst <- 0
jj<-0
k<-0
for(b in 1:x){
  
    if(x >=150000){
      #get out of loop once all data has been processed
      break
    }
  
train_lm = train[1:x,]

#k=k+1
jj[k] <- x 

TraindataLM <- lm(status ~ ., data = train_lm)

#data = Trainset[complete.cases(b),])

predstrn <- predict(TraindataLM, data = train)

predstest <- predict(TraindataLM, data = test) 

R2lmtrn[k] <- 1-sum((train$status-predstrn)^2)/sum((train$status-mean(train$status))^2)

R2lmtst[k] <- 1-sum((test$status-predstest)^2)/sum((test$status-mean(test$status))^2)

k=k+1

x <- x+x

}

#list r-squared for Linear Regression Training and Test data
R2lmtrn
R2lmtst

md <- data.frame(jj, R2lmtrn, R2lmtst)
print(md)

  ggplot(md) +
    geom_point(aes(x = jj, y = R2lmtrn, color = "red")) +
  geom_point(aes(x = jj, y = R2lmtst, color = "green"))+ 
    labs(title = "Performance \n", x = "Data size incrementing by .01", y = "Train_red and Test_green Rsqd", color = "Legend Title\n") +
  scale_color_manual(labels = c("Train", "Test"), values = c("red", "green"))
  

```



subtitle: <h1>Conclusion from the models</h1>


The r-squared value from the <b>Linear Regression model</b> is 0.33506 ~34%. The Training  set performance (r-squared) is .308 ~31%. The Testing set r-squared is 0.

The r-squared value from <b>CART Regression model/ Decision Tree </b> is .318 ~ 32%.
The Training set best performance is 18% while the Test performance is 0.

The r-squared value from the <b>Random Forest Training model </b> at ntree=22 is .214 ~ 21% and as expected, the performance on the Training set is better than the performance on the Testing set.The r-squared value from the Random Forest Testing model is .101~ 10%. 
The word document named "finalproject_randomforest.docx" lists the different r-squared values at ntree values = 15, 10, 5 and 1. At the ntree value of 5, the r-squared or performance is 21% which is slightly better than the performance at ntree = 22 which is .202 ~ 20%.


The model that gave us the best performance is the Linear Regression model because it's R-squared is slightly higher than the CART regression and Random Forest models. Rhe Testing set performance is 31%, which is higher than the training performance of the other models.

