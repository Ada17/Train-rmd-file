---
title: "Predicting Train Arrival Status - On Time or Late"
author: "Adanna Alutu"
date: "June 6, 2017"
output: html_document

---

#Introduction

At the beginning of the project, it was hard to come up with a good data to analyze and predict the outcome. Initially I wanted to work on data from my job but after we coudn't see much dependence among the fields that made sense, my mentor Dr. Shmuel Naaman advised me to scout for data from other internet sites he recommended.

Since I take the train most of the time and experienced delay issues many times that has ranged from 10 mins to 2 hours, I became interested in working on transportation data for trains. This is because I want to experience the process of predicting outcomes which is made possible through Data Science. I want to focus on the steps that will make it possible for me and my mentor Dr Shmuel Naaman to predict the arrival times of the train. The possibility of cutting down the delays experienced in waiting for the train no longer seems to be far fetched.
My mentor agreed with me and the Septa Train data from kaggle website was a good option to work on. There were 3 different datasets available to work on but I chose the "on time performance" which I felt has more relevant features, variables and observations and also has sufficient data for the analysis, tests involved.

The variables in the dataset include:
1. train_id
2. status
3. origin
4. direction
5. next_station
6. timeStamp
7. date

Preparing data for analysis:

Several steps were taken to be able to combine the variables so we could come up with a meaningful interpretation and prediction.

I. We first tried to plot charts with the entire data but the plots were too crowded and blurry to make any sense. The scales were distorted with big units affected by the outliers.

II. GGPlot bar charts were used to plot and observe the trends and statistics summary but the dataset was too huge for the charts.

III. My mentor suggested shuffling the data and taking the first 20percent as sample to work on. Using the formula below, the row-wise shuffling was done first before the column was then shuffled:
```{r, echo=FALSE, eval=FALSE}
#first do row-wise shuffling
train_data_shflr <- train_data[sample(nrow(train_data)),]

train_data_shflc <- train_data_shflr[,sample(ncol(train_data_shflr))]

```

IV. We used the data to fit in several models which include:
    + GGPlot with different combination of the variables.
    + Linear regression model which was used different ways to get the best stastical summary. Including using some of the observations as variables.
    + CART model with focus on the classification method because most of the variables in the data are categorical and the prediction is binary with 0 as "on Time" and 1 as "Late"

Each of these models were implemented because the train dataset contains a mixture of numerical and categorical variables. Converting their types to either numeric or factors wasn't sufficient. To get the benefit of all the variables, it was essential to test these models.

V. Some data manipulations were done which include:
 + splitting some of the original variables nto separate variables. For example, time stamp variable was split into six variables. year, month day, hour, min, seconds.
 + Irrelevant variables were removed or set to null so they would not appear in the dataframe used for the predictions.
 + Some of observations from the wkday and day of month variables were converted to variables and they significantly improved the statistics of the models. The additions however increased the number of variables from 11 to 58.
 + Units attached to the dependent variable observations were removed to enable convesions to different types and allow plotting with only the observations of the same type.
 + The dependent variable "status" observations of "on Time" were replaced with "0" using gsub so that all the observations for the variable will match and easier to manipulate."On time" meant the train arrived as scheduled so it made sense to use "0" to represent no delay.
 
 VI. 
 
#This section shows the summary of the SEPTA train data and the first few records using the head().

```{r, echo=FALSE}

library(tidyr)
library(lubridate)

#train_data <- read.csv("C:/Users/aalutu/Desktop/work/pers/data science/on-time-performance/trainsample_otp.csv")

train_data <- read.csv("C:/Users/aalutu/Desktop/work/pers/data science/on-time-performance/train3000.csv")
#train_data <- read.csv("C:/Users/aalutu/Desktop/work/pers/data science/on-time-performance/fresh3000.csv")
train_data$datets <- as.POSIXct(train_data$timeStamp, format = "%m/%d/%Y %H:%M")
head(train_data$datets)
train_data$wkday <-weekdays(as.Date(train_data$date, format = "%m/%d/%Y"))
head(train_data$wkday)
train_data$month <-month(as.Date(train_data$date, "%m/%d/%Y"))
head(train_data$month)

train_data <- separate(train_data, datets, c("date3","time"), sep = " ")
head(train_data)
train_data <- separate(train_data, time, c("hour","minute"), sep = ":")
head(train_data)

train_data <- separate(train_data, date3, c("date3yr","d3month", "monthday"), sep = "-")
head(train_data)

#train_data$monthday

#remove the date column
train_data$date <- NULL
train_data$date3yr <- NULL
train_data$d3month <- NULL
train_data$timeStamp <- NULL

head(train_data)

train_data$status <- gsub(pattern="min", replacement = "", x = train_data$status, ignore.case = TRUE)
head(train_data$status)
#variable for the cartmodel
#traincartstatus <- train_data$status
#end variable for cartmodel

train_data$status <- gsub(pattern="On Time", replacement = "0", x = train_data$status, ignore.case = TRUE)


#head(train_data)
#summary(train_data)

```

###GGplot graphs used initally to see trends and relationships within the datasets.

####Status - the dependent variable bar chart.It shows the frequency of the delays experienced by passengers at the train station when the train is late.

####From the chart, we can tell that the trains are on time ~50% of the time and late 50% of the time. In this project, we want to predict when to expect the train to be late and when it will be early to avoid waste of time when possible.

```{r}

library("ggplot2")
#set bar levels in descending order

train_var <- train_data$status
train_data2 <- within(train_data,
                      train_var <- factor(train_var,
                                         levels = names(sort(table(train_var),
                                                              decreasing = TRUE))))
trainstat_graph <- ggplot(train_data2, aes(x = train_var)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1)) + coord_cartesian(xlim = c(1, 30)) + scale_x_discrete(name = "Status in minutes")

trainstat_graph


```

#Origin variable bar chart
```{r}

train_var <- train_data$origin

train_data2 <- within(train_data,
                      train_var <- factor(train_var,
                                         levels = sort(table(train_var),
                                                              decreasing = TRUE)))

trainorig_graph <- ggplot(train_data2, aes(x =train_data2$origin)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1)) + coord_cartesian(xlim = c(0, 50)) + scale_x_discrete(name = "Origin")


trainorig_graph
#+ coord_cartesian(xlim = c(0, 50))

```

#Next Station variable bar chart
```{r}

train_var <- train_data$next_station

trainnext_graph <- ggplot(train_data3, aes(x = train_var)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1)) + coord_cartesian(xlim = c(0, 25))+ scale_x_discrete(name = "Next_station")

trainnext_graph

```

#Month bar chart - one of the new variables created from splitting timestamp
```{r}


train_var <- train_data$month


train_data2 <- within(train_data,
                      train_var <- factor(train_var,
                                         levels = names(sort(table(train_var),
                                                              decreasing = TRUE)), ordered = TRUE))
trainmonth_graph <- ggplot(train_data2, aes(x = train_var)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1))+ scale_x_discrete(name = "Numeric Month")

trainmonth_graph

```

### The Weekday chart shows the number of trains that run different days of the week. More trains run during the week and fewer trains on the weekends. The busiest day is Thursday.
```{r}
train_data2 <- within(train_data,
                      train_data$wkday <- factor(train_data$wkday,
                                         levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE))


trainwkday_graph <- ggplot(train_data2, aes(x = train_data$wkday)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1))

trainwkday_graph

```

### This is a chart of all the train ids in descending order. These are all the trains that ran withing the period of a year from our dataset.
```{r}
train_var <- train_data$train_id

trainid_graph <- ggplot(train_data2, aes(x = train_var)) +
  geom_bar() +
   theme(axis.text.x = element_text(angle=90, hjust=1))+ scale_x_discrete(name = "Train_Id") + theme( axis.text.x=element_blank())

trainid_graph

```



###Convert the weekday and monthday observations to columns. The purpose is to increase the number of variables that contribute to the status (delay and on time arrivals) of the train.

### To achieve this, a matrix was used. In this case, the matrix translated the values of the new columns to "0" and "1"."1" was printed when the train travelled in the specified day or month. "0" was used to fill the rest of the observations that the train did not ride during the week day or day of the month. for example, the new column "wkday1" represents "Monday". The value "1" in that column represents the train rides that happened on Mondays.
```{r}
#Added contrasts to print all travel days and all days of the months otherwise some are skipped.

train_data <- cbind(train_data, as.data.frame(model.matrix(~ wkday + monthday, data = train_data, contrasts.arg = list(wkday = contr.treatment(n = 7, contrasts = FALSE), monthday = contr.treatment(n = 31, contrasts = FALSE)))))


#head(train_data)
```

#Linear model statistical summary for Status based on the independent variable x = origin. The linear regression model is one of the models implemented in this project in efforts to predict the delays of the train.

```{r}

train_data$monthday <- as.numeric(train_data$monthday)
#head(traindata5col)
#summary(traindata5col)


##linear model of each graph
library(broom)

glance(summary(lm(log(as.numeric( train_data$status)+1) ~ origin, data = train_data)))

```


### Added more variables to see the effect on the status of the train, which is the variable we are trying to predict.
```{r}

logvar <- log(as.numeric( train_data$status)+1)
glance(summary(lm(logvar ~ origin+ hour + month + monthday + wkday1 + wkday2 + wkday3 + wkday4 + wkday5, data = train_data)))

```
###A regression model of all the 45 variables. The chart after this is an improved model and the final version.

```{r}

glance(summary(lm(as.numeric( train_data$status) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + train_data$next_station + train_data$direction, data = train_data)))


```


###Next is the linear model chat which was used to get better statistics. To achieve a much better R-value > 24%, all significant independent variables were added including the new ones created by the matrix that were converted from observations to variables.

###The original variable count was 11, the addition of the new variables increased the variable count to 58.The improvement of the variables definitely contributed to a better statistics which increased from .06% to 24%.
```{r}
#Use glance() to print only the statistics and not both statistics and train_data summary



glance(summary(lm(logvar ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + train_data$next_station + train_data$direction, data = train_data)))

```

### In this section, the CART model is implemented. The two options considered are Classification and Regression CART models/trees but the reression model is preferred so that the results can be compared with the linear regression model used above. It's like comparing apples to apples or oranges to oranges.

###I found this site very helpful because they explained in detail the conditions for the variables before a succesful model can be achieved - <https://rstudio-pubs-static.s3.amazonaws.com/27179_e64f0de316fc4f169d6ca300f18ee2aa.html>. 

####Prior to finding this site, only the root or just one circle with a number (4.6) was drawn.
```{r}

library(caTools)
set.seed(3000)


#convert train_data$status to numeric
train_data$status = factor(train_data$status)
train_data$status = as.numeric(train_data$status)

#create split
#create split
split = sample.split(train_data$status, SplitRatio = 0.8)

#create training and test set using the subset function

Trainset = subset(train_data, split == TRUE)
Testset = subset(train_data, split == FALSE)

#built CART model

library(rpart)
library(rpart.plot)

#now create the CART model. Use rpart to build a classification tree
#class(Trainset$monthday13)
#use rpart formula to fit the data.

logvar <- log(as.numeric( train_data$status)+1)
TraindataTree2 = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + next_station, data = Trainset)

#prp(TraindataTree2)
#summary(TraindataTree2)
#rsq.rpart(TraindataTree2)
#rsq.rpart

#train_data_regr <- train_data2[,c("status", "origin", "hour", "minute", "month", #"next_station")]

TraindataTree = rpart(Trainset$status ~ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31, data = Trainset)

##plot the tree

#prp(TraindataTree)
#summary(TraindataTree)$r.squared

#rsq.rpart(TraindataTree)

#fitvarUsed <- names(Testset[,1:6])
#preds <- predict(TraindataTree, data = newdata[,c(fitvarUsed)], type = c("prob"))
drpstat <- c("status")
actual <- log(Trainset$status + 1)
fitvarUsed <- !(names(Testset) %in% drpstat)

#fitvarUsed <- !(names(Trainset) %in% drpstat)
#preds <- predict(TraindataTree2, data = newdata[,c(fitvarUsed)])

preds <- predict(TraindataTree2, data = Testset[,fitvarUsed])

#preds <- predict(TraindataTree2, newdata=Testset[,fitvarUsed], type = c("prob"))


#how to plot result and see summary?
sum((actual-preds)^2)

R2 <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R2

```


###Further test to check the performance with 5%, 10%, 30%, 50%, 70%, 80% and all variables at 100% using the Training dataset.
```{r}


TraindataTree2p = rpart(log(Trainset$status +1) ~ origin+ hour, data = Trainset)

preds <- predict(TraindataTree2p, data = Testset[,fitvarUsed])

R2p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R2p

TraindataTree10p = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month, data = Trainset)

preds <- predict(TraindataTree10p, data = Testset[,fitvarUsed])

R10p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R10p



TraindataTree30p = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7, data = Trainset)

preds <- predict(TraindataTree30p, data = Testset[,fitvarUsed])

R30p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R30p

TraindataTree50p = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11, data = Trainset)


preds <- predict(TraindataTree50p, data = Testset[,fitvarUsed])

R50p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R50p


TraindataTree70p = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18, data = Trainset)


preds <- predict(TraindataTree70p, data = Testset[,fitvarUsed])

R70p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R70p


TraindataTree80p = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18+ monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25, data = Trainset)


preds <- predict(TraindataTree80p, data = Testset[,fitvarUsed])

R80p <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R80p

```

###check performance with Testset data
```{r}

actualtst <- log(Testset$status + 1)

TestsetTree2p = rpart(log(Testset$status +1) ~ origin+ hour, data = Testset)

preds <- predict(TestsetTree2p, data = Testset[,fitvarUsed])

R2pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R2pt

TestsetTree10p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month, data = Testset)

preds <- predict(TestsetTree10p, data = Testset[,fitvarUsed])

R10pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R10pt



TestsetTree30p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7, data = Testset)

preds <- predict(TestsetTree30p, data = Testset[,fitvarUsed])

R30pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R30pt

TestsetTree50p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11, data = Testset)


preds <- predict(TestsetTree50p, data = Testset[,fitvarUsed])

R50pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R50pt


TestsetTree70p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18, data = Testset)


preds <- predict(TestsetTree70p, data = Testset[,fitvarUsed])

R70pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R70pt


TestsetTree80p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18+ monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25, data = Testset)


preds <- predict(TestsetTree80p, data = Testset[,fitvarUsed])

R80pt <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R80pt


TestsetTree100p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18+ monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + next_station, data = Testset)


preds <- predict(TestsetTree100p, data = Testset[,fitvarUsed])

R2t <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R2t


```

####Testing max depth on test set
```{r}


TestsetTree100p = rpart(log(Testset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18+ monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + next_station, control = list(maxdepth = 8), data = Testset)


preds <- predict(TestsetTree100p, data = Testset[,fitvarUsed])

R2t <- 1 - (sum((actualtst-preds )^2)/sum((actual-mean(actual))^2))

R2t

```

####Testing max depth on training set

```{r}


TraindataTree2 = rpart(log(Trainset$status +1) ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + next_station, control = list(maxdepth = 10), data = Trainset)


preds <- predict(TraindataTree2, data = Testset[,fitvarUsed])

R2t <- 1 - (sum((actual-preds )^2)/sum((actual-mean(actual))^2))

R2t

```


###Random Forest model implementation. This is the fourth model used in this project to try to come up with a reasonable prediction for the time delay prediction for the train dataset. The result at the end is similar to the Linear regression Cart model.
```{r}

### Random Forest Model # set the seed to make your partition reproducible 
set.seed(123) 
## features for model 

splitlistings <- train_data[c("status", "origin", "hour", "minute", "month", "wkday1", "wkday2","wkday3", "wkday4", "wkday5", "wkday6", "wkday7", "monthday1", "monthday2", "monthday3", "monthday4", "monthday5", "monthday6", "monthday7", "monthday8", "monthday9", "monthday10", "monthday11", "monthday12", "monthday13", "monthday14", "monthday15", "monthday16", "monthday17", "monthday18", "monthday19", "monthday20", "monthday21", "monthday22", "monthday23", "monthday24", "monthday25", "monthday26", "monthday27", "monthday28", "monthday29", "monthday30", "monthday31", "next_station")]

library(caret)
library(randomForest)

inTraining <- createDataPartition(splitlistings$status, p = .8, list = FALSE) 
# save the training and testing sets as data frames 
train_1 <- train_data[ inTraining,] 
test_1 <- train_data[-inTraining,] 
 
 # fit the randomforest model 
model <- train(status ~ origin+ hour + minute + month + wkday1 + wkday2 + wkday3 + wkday4 + wkday5 + wkday6 + wkday7 + monthday1 + monthday2 + monthday3 + monthday4 + monthday5 + monthday6 + monthday7 + monthday8 + monthday9 + monthday10 + monthday11 + monthday12 + monthday13 + monthday14+ monthday15 + monthday16 + monthday17 + monthday18 + monthday19 + monthday20 + monthday21+ monthday22 + monthday23 + monthday24 + monthday25 + monthday26 + monthday27 + monthday28+ monthday29 + monthday30 + monthday31 + next_station, data=train_1, method="rf", metric="RMSE", tuneGrid=expand.grid(.mtry=3), ntree=22, max_depth = 2 , importance=TRUE) 

# what are the important variables (via permutation) 
vi <- varImp(model, type=1) 
plot(vi, top=10) 
# predict the outcome of the training data 
predicted_tr <- predict(model, newdata=train_1, select = -c(status)) 
actual_tr <- train_1$status 
rsq_tr <- 1-sum((actual_tr-predicted_tr)^2)/sum((actual_tr-mean(actual_tr))^2) 
# predict the outcome of the testing data 
predicted <- predict(model, newdata=test_1, select = -c(status)) 
actual <- test_1$status 

rsqd <- 1-sum((actual-predicted)^2)/sum((actual-mean(actual))^2)

rsqd

```

